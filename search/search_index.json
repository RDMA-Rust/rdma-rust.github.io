{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"2025/11/16/why-another-rdma-wrapper/","title":"Why another RDMA wrapper","text":""},{"location":"2025/11/16/why-another-rdma-wrapper/#tl-dr","title":"TL; DR","text":"<p>If you're writing synchronous RDMA code in Rust and you're tired of:</p> <ul> <li>Hand-rolling FFI over <code>ibverbs-sys</code> / <code>rdma-core-sys</code></li> <li>Fighting lifetimes over simple CQ / QP / MR ownership, and</li> <li>Rebuilding <code>rdma-core</code> just to link a tiny binary</li> </ul> <p>Then sideway gives you:</p> <ul> <li>Rust-flavored wrappers over modern ibverbs (<code>ibv_wr_*</code>, <code>ibv_start_poll</code>, Extended CQ / QP)</li> <li>A <code>dlopen</code> based static library so that you don't vendor rdma-core</li> <li>A basic but usable wrapper for <code>librdmacm</code></li> </ul> <p>Checkout our code at https://github.com/RDMA-Rust/sideway</p> <p>This post is about how we built it, what trade-offs we made, and what we learned on the way.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#what-do-we-get","title":"What do we get?","text":"<ol> <li>A Rust flavored wrapper for rdma-core (can saturate 400 Gbps RNIC)</li> <li>Two bug fixes submitted to the rdma-core upstream</li> <li>A \"dummy\" companion library for C/C++ programmer with <code>libibverbs</code> / <code>librdmacm</code> (rdma-core-mummy)</li> <li>A lot of Rust + RDMA programming experience</li> </ol>"},{"location":"2025/11/16/why-another-rdma-wrapper/#whats-wrong-with-current-wrappers","title":"What's wrong with current wrappers?","text":"<p>Most current <code>libibverbs</code> and <code>librdmacm</code> Rust wrappers focus heavily on fancy language features (100% safe, async-first APIs, etc.), and less on being production-ready. They are great for exploration, and provide a lot of insights for our implementation, but awkward when you try to ship a serious RDMA application.</p> <p>When you actually start coding, you often end up:</p> <ul> <li>Reaching for <code>ibverbs-sys</code> (or another bindgen-generated crate)</li> <li>Manually handling error-prone C macros / enum definitions</li> <li>Juggling C types and Rust types at the same time</li> </ul> <p>On top of that, most crates assume you build and link the entire rdma-core project into your binary, or required you've install rdma-core on your compile machine. That pulls in unnecessary dependencies and build complexity.</p> <p>We wanted something that:</p> <ul> <li>Keeps the control path ergonomic in Rust</li> <li>Keeps the data path as close as possible to raw <code>ibverbs</code> to achieve high performance</li> <li>Doesn't require vendoring rdma-core into every build</li> </ul>"},{"location":"2025/11/16/why-another-rdma-wrapper/#how-does-sideway-compare","title":"How does sideway compare?","text":"<p>We are not claiming to replace other crates, different projects have different trade-offs. But for high performance, synchronous RDMA, there are some concrete issues you run into.</p> Crate Requires building rdma-core Wrapping librdmacm Performance issues ibverbs Yes No 1. Poll CQ at a constant batching size2. No batching WRs posting operation3. Unable to specify send flags for moderation or fencing rrddmma Optional No Leave options to user async-rdma No (But specific version required) Yes 1. No batching WRs posting operation2. Unable to specify send flags for moderation for fencing3. Create a CQ for every QP rdma No (But specific version required) No Leave options to user rdma-rs Yes Yes 1. No batching WRs posting operation2. Unable to specify send flags for moderation for fencing Sideway No Yes Focused on modern verbs APIs (<code>ibv_wr_*</code>, CQ / QP Ex), zero-cost abstractions on the data path, and layered design"},{"location":"2025/11/16/why-another-rdma-wrapper/#performance","title":"Performance","text":"<p>To make sure our wrapper doesn't leave performance on the table, we vibe coded a perftest-style benchmarking tool called stride. It's not meant to be a full replacement, just enough for validate that \"Rust + sideway\" can keep up with the classic C tooling.</p> <p>All experiments run on a Nebius GPU-H100-SXM instance with 8 * ConnectX-7 InfiniBand (IB) 400Gbps RDMA NIC</p> Key Value Environment GPU-H100-SXM EU-NORTH1 OS Ubuntu 24.04.3 LTS (Noble Numbat) x86_64 CPU 2 \u00d7 Intel(R) Xeon(R) Platinum 8468 (128) @ 2.10 GHz Kernel 6.11.0-1016-nvidia Memory 1.536 TiB RNIC ConnectX-7 InfiniBand 400 Gbps (firmware 28.39.3004) OFED DOCA 2.9.2 GPU NVIDIA H100 SXM5 80GB NVIDIA GPU Driver 570.195.03 CUDA 12.8 Stride 87b3780 Perftest 20e05b9 <p>We slighty modified stride's code to hardcode the LID in connection setup phase, to make it work with IB. Apart from that, both tools use the same verbs settings.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#what-we-measure","title":"What we measure","text":"<p>We look at three basic scenarios, always with a single RC QP and message sizes from 2B to 8MiB:</p> <ol> <li> <p>Single QP WRITE bandwidth (host / GPU memory)</p> <ul> <li>Host memory: <code>S-1QP-BW</code> (stride) vs <code>P-1QP-BW</code> (perftest)</li> <li>GPU memory (GDR): <code>S-1QP-GDR-BW</code> vs <code>P-1QP-GDR-BW</code></li> </ul> </li> <li> <p>Single QP \"maxed out\" WRITE bandwidth (GPU memory)</p> <ul> <li>GPU memory (GDR), Tx depth 4096, post list 64: <code>S-1QP-GDR-MAX-BW</code> vs <code>P-1QP-GDR-MAX-BW</code></li> <li>To better see small-message behavior, we also add message per second (MPS): <code>S-1QP-GDR-MAX-MPS</code> vs <code>P-1QP-GDR-MAX-MPS</code></li> </ul> </li> <li> <p>Single QP WRITE latency (host / GPU memory)</p> <ul> <li>Host memory: <code>S-1QP-LAT</code> (stride) vs <code>P-1QP-LAT</code> (perftest)</li> <li>GPU memory (GDR): <code>S-1QP-GDR-LAT</code> (perftest does not support WRITE latency with GPU buffers)</li> <li>We also add P99 for jitter: <code>S-1QP-LAT-P99</code> vs <code>P-1QP-LAT-P99</code> vs <code>S-1QP-GDR-LAT-P99</code></li> </ul> </li> </ol>"},{"location":"2025/11/16/why-another-rdma-wrapper/#results-at-a-glance","title":"Results at a glance","text":"<ul> <li>Bandwidth (host memory / GDR)   For a single QP, sideway/stride and perftest track each other closely across   2B\u20138MiB. At large messages, both saturate the 400 Gb/s link:   stride reaches ~380 Gb/s, and perftest is within a few percent of that.</li> </ul> Line TableRaw DataCommand Message Size S-1QP-BW P-1QP-BW S-1QP-GDR-BW P-1QP-GDR-BW 2 0.0676 0.068664 0.077 0.075424 4 0.1352 0.16 0.1546 0.15 8 0.278 0.31 0.3098 0.31 16 0.6217 0.63 0.6201 0.62 32 1.2465 1.26 1.2379 1.24 64 2.4941 2.51 2.4714 2.49 128 4.3461 5.04 4.9527 4.97 256 8.6742 10.12 9.9171 9.93 512 17.3712 20.25 19.7997 19.77 1024 34.7707 40.47 39.6772 39.8 2048 69.6075 80.88 68.9667 79.51 4096 139.2933 164.27 140.3711 160.21 8192 278.4098 305.2 291.2193 320.74 16384 360.9202 343.31 371.0165 371.11 32768 372.1774 358.08 381.8385 382.28 65536 377.9962 365.91 387.1709 387.1 131072 381.4992 370.98 390.377 390.55 262144 382.0199 372.04 391.1473 391.67 524288 381.505 372.35 391.1888 391.9 1048576 381.2342 372.09 391.1909 392.25 2097152 381.307 372.12 391.1919 392.39 4194304 380.6771 371.92 391.1924 392.44 8388608 379.7436 371.82 391.1926 392.44 <pre><code># Perftest server\ntaskset -c 66 ib_write_bw -d mlx5_0 -n 100000 -a -Q 1\n# Perftest client\ntaskset -c 68 ib_write_bw -d mlx5_1 -n 100000 -a 127.0.0.1 -Q 1\n# Perftest server (GDR)\ntaskset -c 66 ib_write_bw -d mlx5_0 -n 100000 -a -Q 1 --use_cuda 4\n# Perftest client (GDR)\ntaskset -c 68 ib_write_bw -d mlx5_1 -n 100000 -a 127.0.0.1 -Q 1 --use_cuda 5\n\n# Stride server\ntaskset -c 66 stride-perf write bw -d mlx5_0 -n 100000 -a --max-msg-size $((8*1024*1024))\n# Stride client\ntaskset -c 68 stride-perf write bw -d mlx5_1 127.0.0.1 -n 1000000 -a --max-msg-size $((8*1024*1024))\n# Stride server (GDR)\ntaskset -c 66 stride-perf write bw -d mlx5_0 -n 100000 -a --max-msg-size $((8*1024*1024)) --use-cuda 4\n# Stride client (GDR)\ntaskset -c 68 stride-perf write bw -d mlx5_1 127.0.0.1 -n 1000000 -a --max-msg-size $((8*1024*1024)) --use-cuda 5\n</code></pre> <ul> <li>Bandwidth (GDR, maxed out)   With a larger <code>tx_depth</code> and <code>post_list</code>, a single QP hits ~392.5 Gbit/s \u2014 essentially   line-rate for a 400 Gbit/s port. For small messages, throughput reaches ~20 Mpps.</li> </ul> Line TableRaw DataCommand Message Size S-1QP-GDR-MAX-BW S-1QP-GDR-MAX-MPS P-1QP-GDR-MAX-BW P-1QP-GDR-MAX-MPS 2 0.3153 19.7076 0.32 19.855156 4 0.632 19.7505 0.64 19.890087 8 1.2636 19.7432 1.28 19.92576 16 2.5247 19.7239 2.54 19.851894 32 5.0515 19.7323 5.07 19.800955 64 10.0584 19.6453 10.11 19.740872 128 20.068 19.5976 20.13 19.659211 256 39.8753 19.4703 39.73 19.400613 512 77.5093 18.9232 78.32 19.121788 1024 147.5344 18.0096 148.25 18.096757 2048 237.1809 14.4764 240.4 14.672663 4096 321.151 9.8007 324.23 9.894732 8192 358.1028 5.4642 361.24 5.512146 16384 391.0239 2.9833 391.81 2.989263 32768 391.1139 1.492 392.17 1.495991 65536 391.1547 0.7461 392.51 0.74865 131072 391.1747 0.3731 392.63 0.374441 262144 391.1837 0.1865 392.6 0.187209 524288 391.1888 0.0933 392.5 0.093579 1048576 391.1903 0.0466 392.46 0.046785 2097152 391.1912 0.0233 392.43 0.023391 4194304 391.1923 0.0117 392.44 0.011696 8388608 391.1925 0.0058 392.44 0.005848 <pre><code># Perftest server\ntaskset -c 66 ib_write_bw -d mlx5_0 -n 102400 -a -Q 1 --report_gbits --use_cuda 4 -l 64 -t 4096\n# Perftest client\ntaskset -c 68 ib_write_bw -d mlx5_1 -n 102400 -a 127.0.0.1 -Q 1 --report_gbits --use_cuda 5 -l 64 -t 4096\n\n# Stride server\ntaskset -c 66 stride-perf write bw -d mlx5_0 -n 102400 -a --max-msg-size $((8*1024*1024)) --use-cuda 4 -l 64 -t 4096\n# Stride client\ntaskset -c 68 stride-perf write bw -d mlx5_1 -n 102400 -a --max-msg-size $((8*1024*1024)) 127.0.0.1 --use-cuda 5 -l 64 -t 4096\n</code></pre> <ul> <li> <p>Latency   For small messages in host memory, perftest\u2019s numbers look better at first glance \u2014 but that\u2019s because we measure slightly different things:</p> <ul> <li>perftest\u2019s WRITE latency is effectively half of a round-trip: client WRITE \u2192 server DMA starts \u2192 server DMA ends (single-trip data + DMA time).</li> <li>stride\u2019s WRITE latency measures data transfer + ACK (client WRITE \u2192 server NIC ACK \u2192 client CQE).</li> </ul> </li> </ul> <p>GPU WRITE latency with stride is slightly higher but still scales cleanly with message size.</p> Line TableRaw DataCommand <p></p> Message Size P-1QP-LAT P-1QP-LAT-P99 S-1QP-LAT S-1QP-LAT-P99 S-1QP-GDR-LAT S-1QP-GDR-LAT-P99 2 3.38 3.47 5.286 5.487 5.412 5.487 4 3.38 3.46 5.322 5.495 5.411 5.479 8 3.38 3.47 5.334 5.495 5.42 5.483 16 3.39 3.47 5.336 5.495 5.418 5.487 32 3.4 3.51 5.329 5.495 5.415 5.483 64 3.4 3.48 5.351 5.495 5.432 5.499 128 3.46 3.57 5.371 5.543 5.45 5.511 256 3.51 3.62 5.421 5.575 5.473 5.539 512 3.57 3.71 5.48 5.619 5.486 5.555 1024 3.61 3.78 5.53 5.683 5.523 5.595 2048 3.75 4.19 5.634 5.743 5.583 5.647 4096 3.93 4.49 5.785 5.895 5.692 5.767 8192 4.13 4.75 5.947 6.163 5.887 6.027 16384 4.53 5.23 6.357 6.499 6.242 6.315 32768 5.07 5.61 6.968 7.087 6.724 6.803 65536 6.11 6.84 7.91 8.063 7.66 7.751 131072 9.53 9.9 10.644 10.807 10.12 10.207 262144 14.66 15.98 14.999 15.239 13.86 13.967 524288 20.12 21.64 20.692 21.599 19.25 19.359 1048576 30.93 33.13 31.571 33.567 29.983 30.079 2097152 53.5 56.62 53.569 57.503 51.442 51.551 4194304 99 103.14 97.649 104.575 94.327 94.463 8388608 189.92 194.66 186.243 196.351 180.107 180.223 <pre><code># Perftest server\ntaskset -c 66 ib_write_lat -d mlx5_0 -n 100000 -a -I 0\n# Perftest client\ntaskset -c 68 ib_write_lat -d mlx5_1 -n 100000 -a 127.0.0.1 -I 0\n\n# Stride server\ntaskset -c 66 stride-perf write lat -d mlx5_0 -n 100000 -a --max-msg-size $((8*1024*1024))\n# Stride client\ntaskset -c 68 stride-perf write lat -d mlx5_1 -n 100000 -a --max-msg-size $((8*1024*1024)) 127.0.0.1\n\n# Stride server (GDR)\ntaskset -c 66 stride-perf write lat -d mlx5_0 -n 100000 -a --max-msg-size $((8*1024*1024)) --use-cuda 4\n# Stride client (GDR)\ntaskset -c 68 stride-perf write lat -d mlx5_1 -n 100000 -a --max-msg-size $((8*1024*1024)) 127.0.0.1 --use-cuda 5\n</code></pre>"},{"location":"2025/11/16/why-another-rdma-wrapper/#design-layered-abstraction","title":"Design: layered abstraction","text":"<p>We treat RDMA abstraction as a layered problem:</p> rdma-corerdma-core-mummyrdma-mummy-syssidewaytrespassYour RPC / storage / inference service dlopenbindgen + manual bindingRust flavored wrappersocket-like communication library"},{"location":"2025/11/16/why-another-rdma-wrapper/#rdma-mummy-sys-the-raw-rdma-core-api-wrapper","title":"<code>rdma-mummy-sys</code> -- the raw rdma-core API wrapper","text":"<p>The first layer is a bindgen-generated + manually corrected library on top of rdma-core (actually <code>rdma-core-mummy</code>, more on that in the <code>dlopen</code> section), This is:</p> <ul> <li>The base for all upper layers</li> <li>Available for users who want direct access to the C APIs with minimal sugar</li> </ul>"},{"location":"2025/11/16/why-another-rdma-wrapper/#sideway-the-rust-flavor-wrapper-over-rdma-core","title":"<code>Sideway</code> -- the Rust flavor wrapper over rdma-core","text":"<p>The second layer is sideway, a Rust-flavored rdma-core wrapper. It:</p> <ul> <li>Re-encapsulates C macros and anonymous enums as Rust enums</li> <li>Uses the builder pattern to make APIs more ergonomic</li> <li>Encapsulates an iterator-style CQ polling API The iterator style is faster than <code>ibv_poll_cq</code> in some projects, and exposes more information: for example, it lets you directly obtain hardware timestamps from CQEs.</li> </ul> <p>We deliberately do not make everything safe:</p> <ul> <li>Wrappers for <code>ibv_reg_mr</code> and <code>ibv_post_send</code> / <code>ibv_wr_*</code> remain <code>unsafe</code></li> <li>Trying to fully encode their safety in this layer would either:<ul> <li>Hurt performance</li> <li>Over-constrain upper layers that need to do unusual things (e.g., GDR with GPU memory)</li> </ul> </li> </ul> <p>Instead, the idea is:</p> <ul> <li>Sideway gives you a clear, zero-cost mapping to modern verbs</li> <li>Upper layers (like <code>trespass</code> or your own library) can build safe memory pools or connection abstractions on top</li> </ul> <p>For user experience, we also provide a unified interface over CQ / CQ Ex and QP / QP Ex. If you've written RDMA before, you must know <code>ibv_post_send</code> and <code>ibv_poll_cq</code>, but you may not have used <code>ibv_wr_*</code> and <code>ibv_start_poll</code>, these newer APIs are more efficient and expose features like hardware timestamps from work completions. Sideway lets you start with the \"classic\" style and move to the extended APIs with minimal changes.</p> <p>Let's compare the traditional <code>ibv_post_send</code>, and <code>ibv_wr_*</code>, and our sideway</p> ibv_post_sendibv_wr_* <pre><code>struct ibv_send_wr wr;\n\nstruct ibv_sge sge = {\n    .addr = local_addr,\n    .length = 4096,\n    .lkey = lkey,\n};\n\nwr.wr_id = 233;\nwr.next = NULL;\nwr.sg_list = &amp;sge;\nwr.num_sge = 1;\nwr.opcode = IBV_WR_RDMA_WRITE;\nwr.send_flags = IBV_SEND_SIGNALED;\nwr.wr.rdma.remote_addr = remote_addr;\nwr.wr.rdma.rkey = rkey;\n\n// Before actual post, you could construct more WRs use `next` to chain them,\n// which could increase performance\nint ret = ibv_post_send(qp, &amp;wr, NULL);\n</code></pre> <pre><code>ibv_wr_start(qp);\n\nqp-&gt;wr_id = 233;\nqp-&gt;wr_flags = IBV_SEND_SIGNALED;\nibv_wr_rdma_write(qp, rkey, remote_addr);\n\nibv_wr_set_sge(qp, lkey, local_addr, 4096);\n\n// Before actual post, you could construct more WRs and then post them at once,\n// which could increase performance\nibv_wr_complete(qp);\n</code></pre> sideway<pre><code>let mut guard = qp.start_post_send();\n\nlet write_handle = guard\n    .construct_wr(233, WorkRequestFlags::Signaled)\n    .setup_write(rmr.rkey(), rmr.get_ptr() as _);\n\nwrite_handle.setup_sge(lmr.lkey(), lmr.get_ptr() as _, 4096);\n\n// Before actual post, you could construct more WRs and then post them at once,\n// which could increase performance\nguard.post()?;\n</code></pre> <p>How about <code>ibv_poll_cq</code> vs <code>ibv_start_poll</code> vs sideway?</p> <p>Although <code>ibv_poll_cq</code> looks cleaner, it doesn't support hardware timestamps and costs more cycles, because it has to copy from CQ to work completions.</p> ibv_poll_cqibv_start_poll <pre><code>struct ibv_wc wc;\n\nwhile (ibv_poll_cq(cq, 1, &amp;wc) &gt; 0) {\n    printf(\"wr_id: %lu, status: %u, opcode: %u\\n\", wc.wr_id, wc.status, wc.opcode);\n}\n</code></pre> <pre><code>int ret = ibv_start_poll(cq, &amp;attr);\n// assume the first ret is not ENOENT\nwhile (ret != ENOENT) {\n    printf(\"wr_id: %lu, status: %u, opcode: %u\\n\", cq-&gt;wr_id, cq-&gt;status, ibv_wc_read_opcode(cq));\n    ret = ibv_next_poll(cq);\n}\nibv_end_poll(cq);\n</code></pre> sideway<pre><code>match cq.start_poll() {\n    Ok(mut poller) =&gt; {\n        while let Some(wc) = poller.next() {\n            println!(\"wr_id: {}, status: {}, opcode: {}\", wc.wr_id(), wc.status(), wc.opcode())\n        }\n    }\n    Err(_) =&gt; {\n        continue;\n    }\n}\n</code></pre> <p>We also invest in better error reporting on the RDMA control path. Instead of a mysterious <code>-EINVAL</code> from the kernel on <code>modify_qp</code>, we:</p> <ul> <li>Tell you what attribute is invalid</li> <li>Surface specific error-based errors (like <code>ETIMEDOUT</code>) as typed Rust errors, making it easier to debug and monitor control-plane behavior</li> </ul> Example on invalid attributes <pre><code>diff --git a/examples/rc_pingpong_split.rs b/examples/rc_pingpong_split.rs\nindex c0d53ed..2d9c61c 100644\n--- a/examples/rc_pingpong_split.rs\n+++ b/examples/rc_pingpong_split.rs\n@@ -250,8 +250,7 @@ impl PingPongContext {\n            .setup_path_mtu(mtu)\n            .setup_dest_qp_num(remote_context.qp_number)\n            .setup_rq_psn(psn)\n-            .setup_max_dest_read_atomic(0)\n-            .setup_min_rnr_timer(0);\n+            .setup_port(ib_port);\n\n        // setup address vector\n        let mut ah_attr = AddressHandleAttribute::new();\n</code></pre> <pre><code>$ cargo run --example rc_pingpong_split -- -d rxe_0 -g 1 -n 1 &amp;\n\n$ cargo run --example rc_pingpong_split -- -d rxe_0 -g 1 -n 1 127.0.0.1\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.09s\n    Running `target/debug/examples/rc_pingpong_split -d rxe_0 -g 1 -n 1 127.0.0.1`\nlocal address: QPN 0x007a, PSN 0xcf45d8, GID 0000:0000:0000:0000:0000:ffff:ac1e:0818\nremote address: QPN 0x0079, PSN 0x3f2d77, GID 0000:0000:0000:0000:0000:ffff:ac1e:0818\n\nthread 'main' panicked at examples/rc_pingpong_split.rs:266:31:\nFailed to modify QP to RTR: ModifyQueuePairError(InvalidAttributeMask { cur_state: Init, next_state: ReadyToReceive, invalid: QueuePairAttributeMask[Port], needed: QueuePairAttributeMask[MinResponderNotReadyTimer, MaxDestinationReadAtomic], source: Os { code: 22, kind: InvalidInput, message: \"Invalid argument\" } })\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n</code></pre> <p>The kernel only tells you <code>EINVAL</code>. sideway tells you that you need to set <code>MinResponderNotReadyTimer</code> and <code>MaxDestinationReadAtomic</code>, and you shouldn't set <code>Port</code> in this transition. It's much easier to fix the code when you have that context.</p> Example on invalid GID / Addr <pre><code>$ cargo run --example rc_pingpong_split -- -d mlx_0 -g 0 -n 1 &amp;\n\n$ cargo run --example rc_pingpong_split -- -d rxe_0 -g 0 -n 1 127.0.0.1\nFinished `dev` profile [unoptimized + debuginfo] target(s) in 0.09s\nRunning `target/debug/examples/rc_pingpong_split -d rxe_0 -g 0 -n 1 127.0.0.1`\n local address: QPN 0x007d, PSN 0xcd1e89, GID fe80:0000:0000:0000:5054:00ff:fe36:7656\nremote address: QPN 0x014b, PSN 0xa0dc7d, GID fe80:0000:0000:0000:fc6b:02ff:fea7:d4bf\n\nthread 'main' panicked at examples/rc_pingpong_split.rs:267:31:\nFailed to modify QP to RTR: ModifyQueuePairError(ResolveRouteTimedout { sgid_index: 0, gid: Gid { raw: [254, 128, 0, 0, 0, 0, 0, 0, 252, 107, 2, 255, 254, 167, 212, 191] }, source: Os { code: 110, kind: TimedOut, message: \"Connection timed out\" } })\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n</code></pre> <p>The result gives us a hint with <code>sgid_index</code> and <code>gid</code>, let's look them up with <code>show_gids</code></p> <pre><code>$ cargo run --example show_gids\nFinished `dev` profile [unoptimized + debuginfo] target(s) in 0.09s\n Running `target/debug/examples/show_gids`\n  Dev   | Port | Index |                   GID                   |    IPv4     |  Ver   | Netdev\n--------+------+-------+-----------------------------------------+-------------+--------+---------\nmlx5_0 |  1   |   0   | fe80:0000:0000:0000:fc6b:02ff:fea7:d4bf |             | RoCEv1 | enp8s0\nmlx5_0 |  1   |   1   | fe80:0000:0000:0000:fc6b:02ff:fea7:d4bf |             | RoCEv2 | enp8s0\nrxe_0  |  1   |   0   | fe80:0000:0000:0000:5054:00ff:fe36:7656 |             | RoCEv2 | enp6s0\n</code></pre> <p>Looks like we're using a link local address for connection setup! That's why it didn't make it. We should set up a global scope IPv6 or IPv4 address with correct routes to make it work. Without a hint of GID index and GID, it takes experience and time to figure out where that <code>ETIMEDOUT</code> came from.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#trepass-the-communication-library-wip","title":"<code>Trepass</code> -- the communication library (WIP)","text":"<p>The third layer is <code>trespass</code>, a communication library that offers a socket-like interface over RDMA, you can think of it as</p> <ul> <li>In the same space as UCX / <code>libfabric</code></li> <li>But intentionally much simpler and focused Its goals:</li> <li>Hide RDMA details such as<ul> <li>Which operations consume recv WQEs</li> <li>How to do flow control</li> <li>How to exchange MR information and other connection information</li> <li>How to do batching for best performance</li> </ul> </li> <li>Let users reuse existing socket programming experience (C / C++ / Rust) to get started quickly.</li> </ul>"},{"location":"2025/11/16/why-another-rdma-wrapper/#rpc-library","title":"RPC library?","text":"<p>The top layer, an RPC framework using RDMA as transport, is intentionally out of scope for us for now. Different applications have very different requirements:</p> <ul> <li>Long running storage systems</li> <li>RPC-heavy microservices</li> <li>ML inference services</li> </ul> <p>Our goal is to provide solid building blocks (rdma-mummy-sys, sideway, trespass) that such frameworks can use.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#where-is-async","title":"Where is async?","text":"<p>You may wonder: where is async? This is Rust, after all.</p> <p>Our view:</p> <ul> <li>RDMA itself is asynchronous</li> <li>Its control path is not usually performance critical</li> <li>Directly integrating verbs with today\u2019s general-purpose async runtimes (especially Tokio) is not straightforward</li> </ul> <p>We think a thread-per-core style runtime is more promising for high-performance RDMA, and sideway is designed so that such a runtime can be built on top</p> <ul> <li>Run a dedicated RDMA worker thread per core</li> <li>Communicate with the rest of the system via channels</li> <li>Keep verbs usage straightforward and predictable.</li> </ul> <p>We may explore this more in the future, but sideway itself is synchronous by design.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#the-magic-of-dlopen","title":"The magic of <code>dlopen</code>","text":"<p>For some historical reasons, <code>libibverbs</code> and some of the providers uses an annoying private symbol mechanism https://github.com/linux-rdma/rdma-core/blob/master/Documentation/versioning.md</p> <p>The idea is to ensure that distributed binaries link against the exact rdma-core version they were built with. While this is understandable, it makes it hard to distribute a single binary across different environments, even when you don\u2019t actually use the private functions.</p> <p>To solve this, we provide a static library called rdma-core-mummy which:</p> <ul> <li>Uses <code>dlopen</code> to load <code>libibverbs</code> and <code>librdmacm</code> at runtime (these two are what you usually need in DCN)</li> <li>So your application only links against our static library instead of all of rdma-core.</li> </ul> <p>You might ask: why not just use <code>libloading</code> in Rust?</p> <p>Because we also want other C/C++ developers to benefit from this approach.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#lifetime-or-not-thats-a-question","title":"Lifetime or not, that's a question","text":"<p>RDMA resources depend on each other</p> <p></p> <p>RDMA resources dependency (from Nugine)</p> <p>At first glance, lifetimes seem like the natural Rust solution:</p> <ul> <li>Express resource relationships in the type system</li> <li>Catch misuse at compile time</li> </ul> <p>We tried that (versions before v0.4.0).</p> <p>When we encapsulated CQ, QP and MR in a <code>WorkerContext</code> (a common C/C++ pattern), lifetime annotations quickly turned into a pain:</p> <ul> <li>The types became complex and fragile</li> <li>We had to use patterns like <code>ouroboros</code> to silence the borrow checker</li> <li>Things that are obviously safe conceptually still made the compiler shout at us The moment where \"safety\" stops helping and starts blocking reasonable designs is the moment we decided we were going too far.</li> </ul> <p>So we chose a simpler model: <code>Arc</code> for most resources.</p> <ul> <li>Users don't have to think about lifetimes</li> <li>Resources are still dropped in a correct order</li> <li>The overhead of refcounting doesn't show up on the data path You can see this in <code>stride</code></li> </ul> <p>Before <code>Arc</code>: https://github.com/RDMA-Rust/stride/commit/c5a6a0c83973bd187f1f4ac57a07942d8d278e23</p> <p>After <code>Arc</code>: https://github.com/RDMA-Rust/stride/commit/fd30aeb4b5b5fa27ee89d1877670058021d670e3</p> Before `Arc`<pre><code>pub struct WorkerContext&lt;'a&gt; {\n    pub queue_pairs: Vec&lt;GenericQueuePair&lt;'a&gt;&gt;,\n    pub send_completion_queue: Rc&lt;RefCell&lt;ExtendedCompletionQueue&lt;'a&gt;&gt;&gt;,\n    pub recv_completion_queue: Option&lt;Rc&lt;RefCell&lt;ExtendedCompletionQueue&lt;'a&gt;&gt;&gt;&gt;,\n    ...\n}\n</code></pre> After `Arc`<pre><code>pub struct WorkerContext {\n    pub queue_pairs: Vec&lt;GenericQueuePair&gt;,\n    pub send_completion_queue: GenericCompletionQueue,\n    pub recv_completion_queue: Option&lt;GenericCompletionQueue&gt;,\n    ...\n}\n</code></pre> <p>Much more readable, and closer to how RDMA is actually used in real systems.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#rusts-unittest-framework-patches-for-upstream","title":"Rust's unittest framework &amp; patches for upstream","text":"<p>To keep sideway healthy, we rely heavily on Rust\u2019s built-in test framework:</p> <ul> <li>Unit tests that can run with or without an RDMA NIC present,</li> <li>CI that exercises SoftRoCE where available. During this process, we found two bugs in rdma-core and upstreamed fixes</li> </ul>"},{"location":"2025/11/16/why-another-rdma-wrapper/#softroce-rxe-cq-bug","title":"SoftRoCE (RXE) CQ bug","text":"<p>When we try to port <code>ibv_rc_pingpong</code> to sideway, it ran fine on Mellanox NICs but behaved strangely on SoftRoCE: we kept polling CQEs with <code>wr_id</code> 0.</p> <p>At first, we thought maybe there is something wrong with our implementation, so we tried to use <code>bpftrace</code> to catch what's happening in the kernel</p> bpftrace script for debugging rxe_qp_tracer.bt<pre><code>#include &lt;rdma/ib_verbs.h&gt;\n\nstruct rxe_pool_elem {\n        void            *pool;\n        void                    *obj;\n        struct kref             ref_cnt;\n        struct list_head        list;\n        struct completion       complete;\n        u32                     index;\n};\n\nenum queue_type {\n        QUEUE_TYPE_TO_CLIENT,\n        QUEUE_TYPE_FROM_CLIENT,\n        QUEUE_TYPE_FROM_ULP,\n        QUEUE_TYPE_TO_ULP,\n};\n\nstruct rxe_queue {\n        void            *rxe;\n        void    *buf;\n        void    *ip;\n        size_t                  buf_size;\n        size_t                  elem_size;\n        unsigned int            log2_elem_size;\n        u32                     index_mask;\n        enum queue_type         type;\n        /* private copy of index for shared queues between\n        * driver and clients. Driver reads and writes\n        * this copy and then replicates to rxe_queue_buf\n        * for read access by clients.\n        */\n        u32                     index;\n};\n\nstruct rxe_sq {\n        int                     max_wr;\n        int                     max_sge;\n        int                     max_inline;\n        spinlock_t              sq_lock; /* guard queue */\n        struct rxe_queue        *queue;\n};\n\nstruct rxe_rq {\n        int                     max_wr;\n        int                     max_sge;\n        spinlock_t              producer_lock; /* guard queue producer */\n        spinlock_t              consumer_lock; /* guard queue consumer */\n        struct rxe_queue        *queue;\n};\n\nstruct rxe_qp {\n        struct ib_qp ibqp;\n        struct rxe_pool_elem elem;\n        struct ib_qp_attr attr;\n        unsigned int valid;\n        unsigned int            mtu;\n        bool                    is_user;\n        void            *pd;\n        void            *srq;\n        void      *scq;\n        void            *rcq;\n        enum ib_sig_type        sq_sig_type;\n        struct rxe_sq           sq;\n        struct rxe_rq           rq;\n};\n\nkprobe:rxe_completer,kprobe:rxe_requester {\n  $qp = (struct rxe_qp *)arg0;\n        printf(\"%s: valid: %u, state: %u\\n\", func, $qp-&gt;valid, $qp-&gt;attr.qp_state);\n        printf(\"%s: max_wr: %u, max_sge: %u, index: %u\\n\", func, $qp-&gt;sq.max_wr, $qp-&gt;sq.max_sge, $qp-&gt;sq.queue-&gt;index);\n}\n\nkretprobe:rxe_completer,kretprobe:rxe_requester {\n        printf(\"%s: return: %d\\n\", func, retval);\n}\n\nkretprobe:rxe_xmit_packet {\n        printf(\"%s: return %d\\n\", func, retval);\n}\n</code></pre> <p>With command below</p> <pre><code>cargo run --example rc_pingpong_split -- -d rxe_0 -g 1 -n 1 &amp;\n\ncargo run --example rc_pingpong_split -- -d rxe_0 -g 1 127.0.0.1 -n 1\n</code></pre> <p>Bpftrace gave us this</p> <pre><code>$ sudo bpftrace ./rxe_qp_tracer.bt\nAttaching 5 probes...\nrxe_requester: valid: 0, state: 3\nrxe_requester: max_wr: 1, max_sge: 8, index: 1\nrxe_requester: return: -11\nrxe_completer: valid: 0, state: 3\nrxe_completer: max_wr: 1, max_sge: 8, index: 1\nrxe_completer: return: -11\nrxe_requester: valid: 1, state: 3\nrxe_requester: max_wr: 1, max_sge: 8, index: 0\nrxe_xmit_packet: return 0\nrxe_requester: return: 0\n...\nrxe_completer: valid: 0, state: 6\nrxe_completer: max_wr: 1, max_sge: 8, index: 1\nrxe_completer: return: -11\n^C\n</code></pre> <p>Which means the <code>rxe_completer</code> only ran finite rounds, and the kernel reported correct number of completions. So we turned to user space driver, after carefully debugging and code review, we finally found the reason and fixed it:</p> <p>rxe: fix completion queue consumer index overrun #1512</p> <p>Then we add it to the CI: E2E rc_pingpong test</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#device-detection-issue","title":"Device detection issue","text":"<p>Another bug appeared after a reboot: one of our tests started to fail. The only difference? SoftRoCE wasn't set up yet. We realized that the machine still had Mellanox VFs, so the test should have passed. But the rdma-core's logic around devices and rdmacm didn't behave as expected.</p> <p>That turned into: librdmacm: prevent NULL pointer access during device initialization</p> <p>Corresponding unit test: rdmacm::communication_manager::tests::test_cm_id_reference_count</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#examples","title":"Examples","text":"<p>If you want to see what real code looks like, check out the examples:</p> <p>https://github.com/RDMA-Rust/sideway/tree/main/examples</p> <p>There are small programs for</p> <ul> <li>Basic RC ping-pong</li> <li>Show GIDs</li> <li>Get device info</li> <li>Check connection setup time with RDMACM</li> </ul> <p>We dogfood sideway through these examples and through <code>stride</code>.</p>"},{"location":"2025/11/16/why-another-rdma-wrapper/#production-status-roadmap","title":"Production status &amp; Roadmap","text":"<ul> <li>Status: v0.Y.Z, the API may still change, but the overall design is fairly stable</li> <li>We are trying to use sideway to build more tools (for example, stride), and adjusting the APIs as we discover better patterns</li> <li>We've tested our APIs on Mellanox CX-5, CX-6, BF3 mini and SoftRoCE, other RNICs haven't be tested yet.</li> <li>Near-term:<ul> <li>Provide UD, UC support</li> <li>Provide CQ Event support</li> <li>Provide Atomic FAA / CAS support</li> <li>Provide thread domain support</li> <li>Plugin system for working with direct verbs</li> <li>More tests, docs, and bug fixes</li> </ul> </li> <li>Long-term:<ul> <li>Complete the trespass communication library</li> <li>Explore a \"thread-per-core\" async runtime on top of sideway</li> </ul> </li> </ul> <p>Let's make Rust network fast with RDMA!</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/rdma/","title":"RDMA","text":""},{"location":"category/rust/","title":"Rust","text":""}]}